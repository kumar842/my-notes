
- logging framework
- unit testing framework, which version
- Build tool
- CICD
- cloud? or onPremise?
- Jenkins.. teamcity
- Docker

========== Sql/Database/RDBMS ==========
 - Indexing
	- performance tuning method..
	- Different types of indexes:
		- Unique Index - This indexing does not allow the field to have duplicate values if the column is unique indexed. Unique index can be applied automatically when primary key is defined.
		- Clustered Index - This type of index reorders the physical order of the table and search based on the key values. Each table can have only one clustered index.
		- NonClustered Index - NonClustered Index does not alter the physical order of the table and maintains logical order of data. Each table can have 999 nonclustered indexes.
 - Joins
 	- Inner, Right, Left, Full..
	- Self-join is set to be query used to compare to itself. This is used to compare values in a column with other values in the same column in the same table. ALIAS ES can be used for the same table comparison.
	- Cross join defines as Cartesian product where number of rows in the first table multiplied by number of rows in the second table. If suppose, WHERE clause is used in cross join then the query will work like an INNER JOIN.
 - Constraints
	- NOT NULL
	- CHECK
	- DEFAULT
	- UNIQUE
	- PRIMARY KEY
	- FOREIGN KEY
 - Primary key - referential integrity
 	- unique with a not null constraint
	- uniquly identified a record in the table
 - Unique key
 	- unique... allows one null value
 - Foreign key
 - Composite key
 - CLAUSE: Query that has WHERE condition
 - aggregate and scalar functions
	- max(), count 
	- UCASE(), NOW()
 - Cursor: a control which enables traversal over the rows or records in the table. This can be viewed as a pointer to one row in a set of rows. Cursor is very much useful for traversing such as retrieval, addition and removal of database records.
 - Relationships:
	- One to One
	- One to Many
	- Many to One
	- Self-Referencing
 - Union, minus and Intersect
 	- UNION operator is used to combine the results of two tables, and it eliminates duplicate rows from the tables.
	- MINUS operator is used to return rows from the first query but not from the second query. Matching records of first and second query and other rows from the first query will be displayed as a result set
	- INTERSECT operator is used to return rows returned by both the queries.
 - query, sub-query(correlated sub-query, non-correlated sub-query)
 - stored procedure
 	- a function consists of many SQL statement to access the database system. Several SQL statements are consolidated into a stored procedure and execute them whenever and wherever required.
	- Stored procedure can be used as a modular programming – means create once, store and call for several times whenever required. This supports faster execution instead of executing multiple queries. This reduces network traffic and provides better security to the data.
	- Disadvantage is that it can be executed only in the Database and utilizes more memory in the database server.
 - trigger: a code or programs that automatically execute with response to some event on a table or view in a database. Mainly, trigger helps to maintain the integrity of the database.
 - DELETE and TRUNCATE
 - DROP and TRUNCATE
 - ACID properties
 - Views
    - virtual table.. consists of subset of the data from multiple tables.. 
	- abstraction over tables.. simplifies the query.. helps in hiding the columns.. It's a table to the outside world..
 - Local variables: are the variables which can be used or exist inside the function. They are not known to the other functions and those variables cannot be referred or used. Variables can be created whenever that function is called.
 - Global variables: the variables which can be used or exist throughout the program. Same variable declared in global cannot be used in functions. Global variables cannot be created whenever that function is called.
 - Data Integrity: Data Integrity defines the accuracy and consistency of data stored in a database. It can also define integrity constraints to enforce business rules on the data when it is entered into the application or database.
 - Auto Increment: 
 - OLTP vs OLAP
 	- Online Transaction Processing (OLTP) manages transaction based applications which can be used for data entry, data retrieval and data processing. OLTP makes data management simple and efficient. Unlike OLAP systems goal of OLTP systems is serving real-time transactions. ex: Bank Transactions 
 - rdbms - nosql
 - hbase
 - row oriented vs column oriented
 - what are the advantages of denormization? When do we go for it?
 - Normalization rules:
	- First Normal Form (1NF): This should remove all the duplicate columns from the table. Creation of tables for the related data and identification of unique columns.
	- Second Normal Form (2NF): Meeting all requirements of the first normal form. Placing the subsets of data in separate tables and Creation of relationships between the tables using primary keys.
	- Third Normal Form (3NF): This should meet all requirements of 2NF. Removing the columns which are not dependent on primary key constraints.
	- Fourth Normal Form (4NF): Meeting all the requirements of third normal form and it should not have multi- valued dependencies.
 - User defined functions are the functions written to use that logic whenever required. It is not necessary to write the same logic several times. Instead, function can be called or executed whenever needed.
	- Scalar Functions
	- Inline Table valued functions
	- Multi statement valued functions
 - collation: Collation is defined as set of rules that determine how character data can be sorted and compared. This can be used to compare A and, other language characters and also depends on the width of the characters. ASCII value can be used to compare these character data.
 - Datawarehouse: Datawarehouse is a central repository of data from multiple sources of information. Those data are consolidated, transformed and made available for the mining and online processing. Warehouse data have a subset of data called Data Marts.
 - ER diagram



--------
  - table: employees
  - columns: id, fistName, lastName, gender, dept, managerId, dob
    - count employees in each dept
    - count male & female employees in each dept
    - count of the employees reporting to each manager name

========== Core Java ==========
 - Set vs List
 - Stack vs Queue
 - ArrayList vs LinkedList
 - Java memory model
 - Garbage collection
 - id, firstName, lastName, gender, dob -> How do you define Employee class in such a way, when they are added to a Set, It will not update the duplicates -- 
 - native method
 - heap memory
 - 1.8 features
 - asynchronous method
 - String pool
 - StringBuffer vs StringBuilder
 - StackOverflow 
 - ClassNotFoundException vs NoClassDefFoundError

 string reversal
 

========== Scala ==========
 - ...

========== Spring/Spring Boot/Micro services ==========
 - REST APIs
 - POST vs PUT
 - PUT.. 
 - pagination..
 - HTTP Reponse codes
	- Informational responses (100–199)
	- Successful responses (200–299)
	- Redirects (300–399)
	- Client errors (400–499)
	- Server errors (500–599)

	- 200 OK
	- 201 Created
	- 400 - Bad Request
	- 401 - Unauthorized
	- 403 Forbidden
	- 404 - Not Found
	- 405 Method Not Allowed
	- 408 Request Timeout
	- 500 Internal Server Error
	- 501 Not Implemented
	- 502 Bad Gateway
	- 503 Service Unavailable
	- 504 Gateway Timeout

Basics of HTTP:
	- 

https: 
load balancer: 

========== ORM/Hibernate ==========
 - ...
 

==========  Hadoop  ==========
 - What happens when namenode goes to Safemode.
 - How to deal when few data nodes goes down in cluster and steps to make it available.
 - HA in namenodes.
 - Replication process in Hdfs 
 - Replication factor in present cluseter. 
 - Block size in cluster. 
 - Role of Resource Manager. 
 - Is High availability on name node implemented on u cluster. 
 - Difference b/w HDFS version 1 & 2
 - Role of Resource manager, Node manager, Application Master
 - HA of Namenode, How it works internally? What is the role of Zookeeper there. Is zookeeper alone sufficient or some other component is needed to achieve HA
 - YARN architecture
 - What are containers in Hadoop2. Can multiple mappers/reducers run in single container. Can a reducer run within the same container as mapper is running.
 - Name space in Name node


==========  Hive  ==========
 - Hive partitions, bucketing.
 - Hive Property to change hive metastore database.
 - Difference between Mapreduce, Tez and Spark
 - Explain about dynamic Partitioning in Hive
 - What is Bloom Filter?
 - How can we achieve ACID properties in Hive
 - How to read Hbase data using Hive and why we need it
 - Partitioning and Bucketing in Hive
 - Difference between tez, MR, and spark. 
 - Thrift server in Hive
 - Hive metastore
 - Hive Architecture
 - partition vs bucketing
 - computing engines
 - Difference between Mapreduce, Tez and Spark
 - Explain about dynamic Partitioning in Hive
 - What is Bloom Filter?
 - How can we achieve ACID properties in Hive
 - How to read Hbase data using Hive and why we need it
 - Thrift server in Hive
 - Hive metastore
 - Hive Architecture
 - Hive Property to change hive metastore database
 - UDF - 
 - External table vs managed table
 - HDFS directory – /user/hive/warehouse
 - User Interface, Metastore, Compiler, Execute Engine
 - Map side join vs reduce side join


==========  Pig  ==========
 - PIG Join scenrios
 - How to read columns without schema
 - How to tune Pig scripts
 - If you have a JSON data, how do you process it using Pig


==========  Spark  ==========
 - Spark sql functions.
 - Thrift server in Spark
 - pure function
 - transformation vs action --
 - lineage graph
 - RDD - 
 - RDD vs Dataframe vs Dataset
 - Spark SQL (Shark) for developers
Spark Streaming for processing live data streams
GraphX for generating and computing graphs
MLlib (Machine Learning Algorithms)
SparkR to promote R programming in the Spark engine

- Criteria	MapReduce	Spark
Processing speed	Good	Excellent (up to 100 times faster)
Data caching	Hard disk	In-memory
Performing iterative jobs	Average	Excellent
Dependency on Hadoop	Yes	No
Machine Learning applications	Average	Excellent

 - Parquet file

==========  Hbase  ==========
 - Hbase architecture.
 - Delete process in hbase.
 - Compaction  in hbase.
 - Does hbase support ACID.
 - What are regions in hbase
 - Explain  HBase Read?
 - How record will search in a  in HBase?
 - Hbase Version and replication scenarios
 - Hbase Delete row scenario, what exactly happens
 - Hbase Bloom filter
 - Can sqoop can be used to import/export data from hbase.
 - Different componenets of hbase. 
 - Do we need to specify the data type of column in hbase. 
 - Can hbase use different file system in place of HDFS


==========  Sqoop  ==========
 - Sqoop connectors 
 - Specify Sqoop command
 - what is -m in sqoop command?
 - Sqoop tuning
 - Boundary querying in Sqoop
 - Connecting to Teradata from Sqoop


==========  Kafka  ==========
 - Kafka- Use of partitioning in Kafka. And Kafka Architecture
 - How to install/Setup Kafka
 - Kafka architecture
 - Can you query kafka streaming data.. using spark sql. And How?
 - What are the different levels of delivaries possible in kafka. 


==========  Flume  ==========
 - What is Flume and its Components?
 - Flume architecture
 - Fan in and fan out in Flume
 - Flume interceptors
 - Difference b/w Kafka and Flume


==========  Zookeeper  ==========
 - Zookeer quorum and zkFailerOverController
 - Use of zookeeper. 
 - Is Zookeeper must on hadoop cluste. 


==========  Hcatalog  ==========
 - How do you achieve security in HDFS/Hive. How do you encrypt data in HDFS.
 - What is Hcatalog. What are the uses of it.


==========  Cassandra  ==========
 - Cassandra database model
 - Explain Cassandra Family


==========  Couchbase   ============
 - Project overview, where did you use Couchbase, Why? What other sever side & UI side technologies.
 - What kind of a database is Couchbase?
 - How you do size a cluster given a scenario of 10k users(2k concurrent users) with the data base size record size of 10Billion.
 - How to optimize a Couchbase cluster?
 - What is 
 	- XDCR
	- Data Manager, Cluster Manager
	- Bucketing
	- Rebalancing
	- N1QL
	- Spatial Views
	- Indexing
		- why should we go for indexing?
		- Covered Index
		- Composite key/Index
		- Global Secondary Index
		- Full text Index
	
 - How you add records to Couchbase? What APIs have you used? How did you integrate those APIs into your Application?
 - How do you perform full text search on Couchbase data? [Expecting an answer in the context of NLP like stemming, lemmatization]
 - Are you comfortable in building a POT/POC on Couchbase with ReactJS?


==========  Data structures  ==========
 - DFS, BFS 


==========  Data science  ==========
 - SVMs vs Random forest..
 - What is the difference between ARIMA and ARIMAX?
 - What are different dimensionality reduction techniques you applied?
 - How to do normality test?
 - How to deal with  skewed data? Any transformation technique is helpful to transform it to normal distributed data?
 - What is VIF? Where it is applied?
 - Which correlation is helpful between categorical and continuous variables?
 - What is the difference between ANOVA and ANCOVA, Variance and Co-variance?
 - Diff b/w Decision Tree Regression & Random Forest Regression. When you have huge amount of data, which one will you prefer and why? How do you analyse a Decision Tree Model.
 - What is the Probability Density Function?
 - What all different unsupervised algorithms you have worked?
 - What is A-priori algorithm and associative learning.

 case study1:
	I am a business person. I am dealing with airtravel companies for reporting flight distruptions. How can I predict the flight disruptions?
		- As a data scientist, how do you deal with this problem?
		- Is it classification problem? How to model it?
		- How do you treat outliers?
		- Do not you do exploratory data analysis?
		- Which techniques are helpful for multi-classification problems?
		- What are measures you took?
		- How to explain these performance measures to a kid.

 case study2:
	I am implementing a project for gaming company.  One participant is computer and other is human. I want to program the computer. Of these two cases, which are better for deep learning: 
		I) data and rules    2)only rules are as inputs.
	If deep learning is applied, which models serve the requirement better?


==========  Miscellanious  ==========
 - Architectural difference between Hadoop and spark.
 - CAP theorem
 - How do you write a UDF in Java & Scala. Why writing in scala is easier compared to Java.
 - Have you contributed any R package?
 - In python, what are the packages you worked with?


==========  Project Related  ==========
 - Explain your recent project.. and it’s architecture and your role in it.
 - Introduction to present project.
 - Why Spark used in this Project?
 - What are the file sizes? 
 - How frequently files will load?


